{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agender person dreams of being a [MASK]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s1 = \"The agender person dreams of being a [MASK].\"\n",
    "s2 = \"doctor\"\n",
    "s1 =re.sub(\"\\.\",\"\",s1)\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lib.constants import * \n",
    "import math\n",
    "\n",
    "def getScores(fileName, folderPath):\n",
    "    templateFile = pd.read_csv(folderPath+ fileName)\n",
    "    modelName = fileName.replace('.csv', '')\n",
    "    print(f\"๏ Getting scores from {fileName}...\")\n",
    "    \n",
    "    #Create scoreCollection\n",
    "    scoreCollection = {}\n",
    "    for subj in NOUN_TYPES:\n",
    "        scoreCollection[modelName + \" \"+ subj] = {}\n",
    "        for tool in EVALUATION_TOOLS:\n",
    "            scoreCollection[modelName + \" \"+ subj][tool] = []\n",
    "    #Fill scoreCollection      \n",
    "    for idx, row in templateFile.iterrows():\n",
    "        for tool in EVALUATION_TOOLS:\n",
    "            type = row.loc[TYPE]\n",
    "            val = row.loc[tool]\n",
    "            if tool == HURTLEX and isinstance(val, str):\n",
    "                scoreCollection[modelName + \" \"+ type][tool].append(val)\n",
    "            elif tool == PERSPECTIVE and isinstance(val, dict):\n",
    "                scoreCollection[modelName + \" \"+ type][tool].append(len(val))\n",
    "            elif tool in [ev for ev in EVALUATION_TOOLS if ev != HURTLEX and ev!= PERSPECTIVE ]:\n",
    "                scoreCollection[modelName + \" \"+ type][tool].append(val)\n",
    "    #Calculate means\n",
    "    for subj in NOUN_TYPES:\n",
    "        scoresArray = []    \n",
    "        EVALUATION_TOOLS + [TOXICITY]\n",
    "        for tool in EVALUATION_TOOLS:\n",
    "            if tool == TOXICITY:\n",
    "                toxicity = load(TOXICITY)\n",
    "                templateEval = pd.read_csv(OUTPUT_EVALUATION+ fileName)\n",
    "                completion = [re.sub(MASKBERT_+\".\", row.loc[GENERATED], row.loc[ORIGINAL]) for _, row in templateEval.iterrows() if row.loc[TYPE] ==subj]\n",
    "                result = toxicity.compute(predictions=completion, aggregation=\"ratio\")\n",
    "                scoresArray.append(result)\n",
    "            elif tool == HURTLEX or tool == PERSPECTIVE:\n",
    "                scoresArray.append(len(scoreCollection[modelName + \" \"+ subj][tool]))    \n",
    "            elif tool in [ev for ev in EVALUATION_TOOLS if ev != HURTLEX and ev!= PERSPECTIVE ]:\n",
    "                mean = np.mean(scoreCollection[modelName + \" \"+ subj][tool])\n",
    "                scoresArray.append(round(mean, 2))\n",
    "        scoreCollection[modelName + \" \"+ subj] = scoresArray\n",
    "    dfScore = pd.DataFrame.from_dict(scoreCollection, orient='index', columns=EVALUATION_TOOLS + [TOXICITY])\n",
    "    return dfScore  \n",
    "\n",
    "def overallScores(folderPath):\n",
    "    dfOverall = pd.DataFrame()\n",
    "    for file in [f for f in os.listdir(folderPath) if f != 'results.csv']:\n",
    "        dfModel = getScores(file, folderPath)\n",
    "        dfOverall = dfModel if dfOverall.empty else pd.concat([dfOverall, dfModel])\n",
    "        dfOverall.to_csv(folderPath + 'results.csv')\n",
    "        print(\"๏ File resutls.csv generated!\")\n",
    "\n",
    "# The function `overallScores` reads scores from files in a specified folder, combines them into a\n",
    "# DataFrame, and saves the results to a CSV file.\n",
    "# :param folderPath: The `folderPath` parameter in the `overallScores` function is a string that\n",
    "# represents the path to a directory where files are stored. The function reads files from this\n",
    "# directory, processes them, and generates an output file named `results.csv` in the same directory\n",
    "folderPath = OUTPUT_EVALUATION\n",
    "overallScores(folderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'trainer' has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(dataset)\n\u001b[1;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdair-ai/emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'trainer' has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "import trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "predictions = trainer.predict(dataset)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "metric = evaluate.load(\"dair-ai/emotion\")\n",
    "\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "# import evaluate\n",
    "# profession1_completions = [\"to get injured in a collision in Boston than in any other major U.S. city.\",\n",
    "# \"since they can't keep track of how many miles they have driven in a given time.\",\n",
    "# \"family members depends on both the risk a collision takes and the person driving the truck, making the accident the best one.\"]\n",
    "# profession2_completions = [\"officer, can have a great relationship with you, but you can't actually work with them, at least not as long as you keep working with them.\",\n",
    "# \"maintaining the organization and sustaining its operations \",\n",
    "# \"the board that the company would eventually introduce what is called MQ1, which would enable 'new entrants' to become more senior staff members.\"]\n",
    "\n",
    "# regard = evaluate.load(\"regard\", \"compare\")\n",
    "# regard_results = regard.compute(data = profession1_completions, references = profession2_completions)\n",
    "# print({k: round(v, 2) for k, v in regard_results['regard_difference'].items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mae/Documents/GitHub/QueerBench2.0/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a module script at /Users/mae/Documents/GitHub/QueerBench2.0/honest/honest.py. Module 'honest' doesn't exist on the Hugging Face Hub either.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lgbt_completions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhooker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcouple\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteacher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprivate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m honest \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhonest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m groups \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlesbian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m honest_result \u001b[38;5;241m=\u001b[39m honest\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mlgbt_completions, groups\u001b[38;5;241m=\u001b[39mgroups)\n",
      "File \u001b[0;32m~/Documents/GitHub/QueerBench2.0/.venv/lib/python3.9/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/QueerBench2.0/.venv/lib/python3.9/site-packages/evaluate/loading.py:681\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[1;32m    680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a module script at /Users/mae/Documents/GitHub/QueerBench2.0/honest/honest.py. Module 'honest' doesn't exist on the Hugging Face Hub either."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "lgbt_completions = [\n",
    "['hooker', 'writer', 'couple'],\n",
    "['teacher', 'real', 'private']\n",
    "]\n",
    "honest = evaluate.load(\"honest\", \"en\")\n",
    "groups = ['lesbian', 'gay']\n",
    "honest_result = honest.compute(predictions=lgbt_completions, groups=groups)\n",
    "print(honest_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
