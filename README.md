# QueerGen: Evaluating Representational Harm Toward Queer Identities in Language Models

**QueerGen** is a framework for evaluating how large language models (LLMs) represent queer and non-normative identities. It uses controlled prompt templates and automated analysis to assess **sentiment**, **regard**, **toxicity**, and **lexical diversity** across completions generated by different models for *unmarked*, *non-queer-marked*, and *queer-marked* subjects.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ templates.csv
â”‚   â”œâ”€â”€ subjects.csv
â”‚   â”œâ”€â”€ markers.csv
â”‚   â””â”€â”€ template_complete.csv        # Auto-generated prompts
â”‚
â”œâ”€â”€ generations/                     # Generated completions (by model & prompt)
â”œâ”€â”€ evaluations/                     # Sentiment, regard, toxicity, diversity scores
â”œâ”€â”€ graphs/                          # Generated charts
â”œâ”€â”€ tables/                          # CSV tables used in paper
â”‚
â”œâ”€â”€ env_config.sh                    # Environment + model setup
â”œâ”€â”€ main.py                          # Runs full pipeline
â”œâ”€â”€ graphs.ipynb                     # Regenerate plots and tables (Jupyter)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ template.py                  # Prompt creation
â”‚   â”œâ”€â”€ models.py                    # Completion generation (API + local)
â”‚   â”œâ”€â”€ evaluation.py                # Evaluation metrics
â”‚   â””â”€â”€ lib.py                       # Utilities
â””â”€â”€ .env                             # API keys (required)
```

---

## âš™ï¸ Installation

Install all required libraries and download models by running:

```bash
bash env_config.sh
```

This script will:
- Install Python dependencies
- Download local Ollama models (e.g., LLaMA 3, Gemma, DeepSeek)
- Set up necessary folder structure
- Clone regard evaluation models

---

## â–¶ï¸ Run the Full Pipeline

To run **all steps** â€” from prompt generation to model completion and evaluation:

```bash
python main.py
```

This will:
- Create all templates (`template_complete.csv`)
- Generate sentence completions for all models and prompts
- Evaluate completions for sentiment, regard, toxicity, and diversity
- Save all outputs to the appropriate folders

---

## ğŸ“Š Plotting & Table Generation

To generate or update all the visualizations and tables used in the analysis, open:

```bash
graphs.ipynb
```

This Jupyter notebook reads evaluation data and generates:
- All charts in `graphs/`
- All result tables in `tables/`

---

## ğŸ” API Configuration

The `.env` file must contain the required API keys for the external models and services:

```env
OPENAI_API_KEY=your_openai_key
GENAI_API_KEY=your_genai_key
DEEPSEEK_API_KEY=your_deepseek_key
PERSPECTIVE_API_KEY=your_perspective_key
```

| Key                   | Used For                 |
|----------------------|--------------------------|
| `OPENAI_API_KEY`     | GPT-4, GPT-4o-mini       |
| `GENAI_API_KEY`      | Gemini 2.0 Flash         |
| `DEEPSEEK_API_KEY`   | DeepSeek R1              |
| `PERSPECTIVE_API_KEY`| Toxicity evaluation      |

---

## ğŸ§  Script Functionality Overview

- `src/models.py`: Handles prompt completion using both open- and closed-access LLMs via APIs or Ollama. Each model is wrapped to ensure consistent generation and output format.

- `src/evaluation.py`: Computes automatic fairness metrics for all generated sentences, including:
  - Sentiment (VADER)
  - Regard (via pre-trained classifier)
  - Toxicity (Perspective API)
  - Lexical Diversity (type-token ratio)

---

**License**: Academic research only. Please cite the corresponding paper if you use QueerGen in your work.
